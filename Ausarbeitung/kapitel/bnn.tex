\chapter{Das Netzwerk}
\section{Aktivierungsfunktion}
%TODO HtanH im Kontext von Expolding Gradient
\section{Binärer Linear-Layer}
\section{BatchNorm}
\section{Auswertung}
Um am Ende das Ergebnis des Netzwerkes auszuwerten, müssen die Ergebnisse des letzten \textit{Linear-Layers} normalisiert werden. Diese normalisierung entscheidet, ob ein Neuron feuert oder nicht. Da bei der Anwendung des MNIST-Datensatzes immer nur das Neuron ausgewählt werden sollte, da immer nur eine Zahl auf einem Bild ist, wird dieses am ende über die $argmax(x)$ Funktion. Damit ist die erkannte Zahl immer das aktivste Neuron.\\
Für die Normalisierung der Daten wird hier die \textit{logsoftmax(x)} Funktion verwendet.
\subsection{logsoftmax vs. softmax}
\section{Verluste durch binäre Linear-Layer}
Durch die binarisierung der \textit{Linear-Layer} ist zu vermuten, dass diese, im Vergleich zu normalen \textit{Linear-Layer}, etwas schlechter performen. Dies ist der Fall, da die Anzahl der möglichen Kantengewichte stark, auf Null und Eins, eingeschränkt ist. 
\begin{figure}[h]
	\centering
	\begin{tabular}{|c|c|c|}\hline
		Durchgang&binarär&normal\\\hline
		1&
	\end{tabular}
\end{figure}


Trainiert wurde hier das gleiche Netzwerk, ein mal mit binären \textit{Linear-Layern}, das andere mal mit normalen \textit{Linear-Layer}. Jedes Netzwerk wurde für 50 Epochen trainiert, bevor die Genauigkeit ausgewertet wurde. Um sicher zu gehen, ob die Genauigkeit gegen diesen Wert konvergiert, wurde jede Messung fünf mal wiederholt.