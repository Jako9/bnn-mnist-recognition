\chapter{Das Netzwerk}
\section{Aktivierungsfunktion}
%TODO HtanH im Kontext von Expolding Gradient
\section{Binärer Linear-Layer}
\section{BatchNorm}

\section{Auswertung}
Um am Ende das Ergebnis des Netzwerkes auszuwerten, müssen die Ergebnisse des letzten \textit{Linear-Layers} normalisiert werden. Diese Normalisierung entscheidet, ob ein Neuron feuert oder nicht. Für die Normalisierung der Daten wird hier die \textit{logsoftmax(x)} Funktion verwendet. Durch \textit{softmax(x)} werden die Aktivierungen der letzten Schicht so normalisiert, dass ihre Summe eins ergibt. Dies ist mit Wahrscheinlichkeiten zu vergleichen, mit der das Bild die jeweilig zugeordnete Zahl widerspiegelt. Um bessere Ergebnisse in Kombination mit der Verlustfunktion, \textit{negative log likelihood loss}, zu erzielen, wird die \textit{logsoftmax}-Funktion verwendet.\\
Da bei der Anwendung des MNIST-Datensatzes immer nur das Neuron ausgewählt werden sollte, da immer nur eine Zahl auf einem Bild abgebildet ist, wird am bei der Auswertung über die \textit{argmax}-Funktion das aktivste Neuron ausgewählt. Dieses entspricht dann der Zahl, die am wahrscheinlichsten abgebildet ist.

\subsection{logsoftmax vs. softmax}
\section{Verluste durch binäre Linear-Layer}
Durch die binarisierung der \textit{Linear-Layer} ist zu vermuten, dass diese, im Vergleich zu normalen \textit{Linear-Layer}, etwas schlechter performen. Dies ist der Fall, da die Anzahl der möglichen Kantengewichte stark, auf Null und Eins, eingeschränkt ist. 
\begin{figure}[h]
	\centering
	\begin{tabular}{|c|c|c|}\hline
		Durchgang&binarär&normal\\\hline
		1&88.29&97.43\\\hline
		2&87.32&96.98\\\hline
		3&87.19&97.2\\\hline
	\end{tabular}
\caption{Verlustmessung für den \textit{Linear-Layer}}
\label{fig:linar-layer-verluste}
\end{figure}


Trainiert wurde hier das gleiche Netzwerk, ein mal mit binären \textit{Linear-Layern}, das andere mal mit normalen \textit{Linear-Layer}. Jedes Netzwerk wurde für 50 Epochen trainiert, bevor die Genauigkeit ausgewertet wurde. Um sicher zu gehen, ob die Genauigkeit gegen diesen Wert konvergiert, wurde jede Messung drei mal wiederholt.\\
Wie in Abbildung \ref{fig:linar-layer-verluste} zu sehen, leidet die Genauigkeit des Netzwerkes beachtlich unter der Binarisierung der \textit{Linear-Layer}. Der Mittelwert für das Training mit normalem \textit{Linear-Layer} ist hierbei $97,2\%$, während bei binären Schichten ein Durchschnitt von $87,6\%$ erreicht wird. Es ist klar zu sehen, dass die Einschränkung der Gewichte auf Null und Eins und der damit einhergehende Granularitätsverlust, sich stark auf die Genauigkeit des Netzwerkes, bei gleicher Größe, auswirken. 
